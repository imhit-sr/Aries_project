{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8695315,"sourceType":"datasetVersion","datasetId":5214503}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-18T04:37:47.201261Z","iopub.execute_input":"2024-06-18T04:37:47.201648Z","iopub.status.idle":"2024-06-18T04:37:48.261826Z","shell.execute_reply.started":"2024-06-18T04:37:47.201616Z","shell.execute_reply":"2024-06-18T04:37:48.260733Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import BertTokenizer, BertForQuestionAnswering\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:37:51.402182Z","iopub.execute_input":"2024-06-18T04:37:51.403296Z","iopub.status.idle":"2024-06-18T04:37:57.051336Z","shell.execute_reply.started":"2024-06-18T04:37:51.403220Z","shell.execute_reply":"2024-06-18T04:37:57.050323Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"is_cuda = torch.cuda.is_available()\n\n# If we have a GPU available, we'll set our device to GPU.\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:37:57.053272Z","iopub.execute_input":"2024-06-18T04:37:57.054289Z","iopub.status.idle":"2024-06-18T04:37:57.060446Z","shell.execute_reply.started":"2024-06-18T04:37:57.054247Z","shell.execute_reply":"2024-06-18T04:37:57.059136Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"GPU not available, CPU used\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the path to the SQuAD 2.0 training data\npath = \"/kaggle/input/squad-dataset-mee/train-v2.0.json\"\n\n# Load and preprocess the SQuAD 2.0 data\ndef load_squad_data(path):\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    texts = []\n    questions = []\n    answers = []\n\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    texts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n\n    return texts[0:10850], questions[0:10850], answers[0:10850]\n\n# Preprocess the data to find answer start and end positions\ntrain_texts, train_queries, train_answers = load_squad_data(path)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:37:58.981994Z","iopub.execute_input":"2024-06-18T04:37:58.982383Z","iopub.status.idle":"2024-06-18T04:38:00.662614Z","shell.execute_reply.started":"2024-06-18T04:37:58.982354Z","shell.execute_reply":"2024-06-18T04:38:00.661646Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\n# Give the path for SQuAD 2.0 validation data\npath = Path('/kaggle/input/squad-dataset-mee/dev-v2.0.json')\n\n# Load and preprocess the SQuAD 2.0 data\ndef load_squad_data(path):\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    texts = []\n    questions = []\n    answers = []\n\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    texts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n\n    return texts[0:2537], questions[0:2537], answers[0:2537]\n\n# Preprocess the data to find answer start and end positions\nval_texts, val_queries, val_answers = load_squad_data(path)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:38:07.517973Z","iopub.execute_input":"2024-06-18T04:38:07.518393Z","iopub.status.idle":"2024-06-18T04:38:07.654505Z","shell.execute_reply.started":"2024-06-18T04:38:07.518352Z","shell.execute_reply":"2024-06-18T04:38:07.653292Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"for answer, text in zip(train_answers, train_texts):\n    real_answer = answer['text']\n    start_idx = answer['answer_start']\n    # Get the real end index\n    end_idx = start_idx + len(real_answer)\n\n    # Deal with the problem of 1 or 2 more characters\n    if text[start_idx:end_idx] == real_answer:\n        answer['answer_end'] = end_idx\n    # When the real answer is more by one character\n    elif text[start_idx-1:end_idx-1] == real_answer:\n        answer['answer_start'] = start_idx - 1\n        answer['answer_end'] = end_idx - 1\n    # When the real answer is more by two characters\n    elif text[start_idx-2:end_idx-2] == real_answer:\n        answer['answer_start'] = start_idx - 2\n        answer['answer_end'] = end_idx - 2","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:38:11.282705Z","iopub.execute_input":"2024-06-18T04:38:11.283056Z","iopub.status.idle":"2024-06-18T04:38:11.299395Z","shell.execute_reply.started":"2024-06-18T04:38:11.283030Z","shell.execute_reply":"2024-06-18T04:38:11.298296Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"\nfor answer, text in zip(val_answers, val_texts):\n    real_answer = answer['text']\n    start_idx = answer['answer_start']\n    # Get the real end index\n    end_idx = start_idx + len(real_answer)\n\n    # Deal with the problem of 1 or 2 more characters\n    if text[start_idx:end_idx] == real_answer:\n        answer['answer_end'] = end_idx\n    # When the real answer is more by one character\n    elif text[start_idx-1:end_idx-1] == real_answer:\n        answer['answer_start'] = start_idx - 1\n        answer['answer_end'] = end_idx - 1\n    # When the real answer is more by two characters\n    elif text[start_idx-2:end_idx-2] == real_answer:\n        answer['answer_start'] = start_idx - 2\n        answer['answer_end'] = end_idx - 2","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:38:32.199428Z","iopub.execute_input":"2024-06-18T04:38:32.200734Z","iopub.status.idle":"2024-06-18T04:38:32.209408Z","shell.execute_reply.started":"2024-06-18T04:38:32.200685Z","shell.execute_reply":"2024-06-18T04:38:32.208172Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:38:43.702770Z","iopub.execute_input":"2024-06-18T04:38:43.703159Z","iopub.status.idle":"2024-06-18T04:38:47.060526Z","shell.execute_reply.started":"2024-06-18T04:38:43.703125Z","shell.execute_reply":"2024-06-18T04:38:47.059515Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89b45168d125426db02ec19741615062"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcc97e8bfcd409bb9f5f2798f9d03b3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196e155cc1d242ad82fd90d8483db176"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b259ede9ecd5408ab84f640398421456"}},"metadata":{}}]},{"cell_type":"code","source":"train_encodings = tokenizer(train_texts, train_queries, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, val_queries, truncation=True, padding=True)\n# If necessary (based on the maximum length), truncate both text and query sequences to fit the specified length.","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:44:20.769908Z","iopub.execute_input":"2024-06-18T04:44:20.770377Z","iopub.status.idle":"2024-06-18T04:44:25.496586Z","shell.execute_reply.started":"2024-06-18T04:44:20.770341Z","shell.execute_reply":"2024-06-18T04:44:25.495553Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_encodings['input_ids'][0].count(102)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:52:19.073681Z","iopub.execute_input":"2024-06-18T04:52:19.074053Z","iopub.status.idle":"2024-06-18T04:52:19.083198Z","shell.execute_reply.started":"2024-06-18T04:52:19.074025Z","shell.execute_reply":"2024-06-18T04:52:19.082098Z"},"trusted":true},"execution_count":25,"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"2"},"metadata":{}}]},{"cell_type":"code","source":"print(\"Passage: \",train_texts[0])\nprint(\"Query: \",train_queries[0])\nprint(\"Answer: \",train_answers[0])\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:42:08.604902Z","iopub.execute_input":"2024-06-18T04:42:08.605327Z","iopub.status.idle":"2024-06-18T04:42:08.610861Z","shell.execute_reply.started":"2024-06-18T04:42:08.605294Z","shell.execute_reply":"2024-06-18T04:42:08.609688Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Passage:  Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".\nQuery:  When did Beyonce start becoming popular?\nAnswer:  {'text': 'in the late 1990s', 'answer_start': 269, 'answer_end': 286}\n","output_type":"stream"}]},{"cell_type":"code","source":"def add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    \n    count = 0\n    \n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n        \n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n            \n        # if end position is None, the 'char_to_token' function points to the space after the correct token, so add - 1\n        if end_positions[-1] is None:\n            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n            \n            # if end position is still None the answer passage has been truncated\n            if end_positions[-1] is None:\n                count += 1\n                end_positions[-1] = tokenizer.model_max_length\n    print(count)\n    \n    # Update the data in dictionary\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:53:44.689980Z","iopub.execute_input":"2024-06-18T04:53:44.690388Z","iopub.status.idle":"2024-06-18T04:53:44.748932Z","shell.execute_reply.started":"2024-06-18T04:53:44.690359Z","shell.execute_reply":"2024-06-18T04:53:44.747864Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"2\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"class SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:53:52.173998Z","iopub.execute_input":"2024-06-18T04:53:52.175021Z","iopub.status.idle":"2024-06-18T04:53:52.181088Z","shell.execute_reply.started":"2024-06-18T04:53:52.174981Z","shell.execute_reply":"2024-06-18T04:53:52.179959Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_dataset = SquadDataset(train_encodings)\nval_dataset = SquadDataset(val_encodings)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:53:58.574747Z","iopub.execute_input":"2024-06-18T04:53:58.575282Z","iopub.status.idle":"2024-06-18T04:53:58.581950Z","shell.execute_reply.started":"2024-06-18T04:53:58.575219Z","shell.execute_reply":"2024-06-18T04:53:58.580293Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"(train_dataset[0])","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:54:12.870542Z","iopub.execute_input":"2024-06-18T04:54:12.871477Z","iopub.status.idle":"2024-06-18T04:54:12.925112Z","shell.execute_reply.started":"2024-06-18T04:54:12.871441Z","shell.execute_reply":"2024-06-18T04:54:12.923857Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([  101, 20773, 21025, 19358, 22815,  1011,  5708,  1006,  1013, 12170,\n         23432, 29715,  3501, 29678, 12325, 29685,  1013, 10506,  1011, 10930,\n          2078,  1011,  2360,  1007,  1006,  2141,  2244,  1018,  1010,  3261,\n          1007,  2003,  2019,  2137,  3220,  1010,  6009,  1010,  2501,  3135,\n          1998,  3883,  1012,  2141,  1998,  2992,  1999,  5395,  1010,  3146,\n          1010,  2016,  2864,  1999,  2536,  4823,  1998,  5613,  6479,  2004,\n          1037,  2775,  1010,  1998,  3123,  2000,  4476,  1999,  1996,  2397,\n          4134,  2004,  2599,  3220,  1997,  1054,  1004,  1038,  2611,  1011,\n          2177, 10461,  1005,  1055,  2775,  1012,  3266,  2011,  2014,  2269,\n          1010, 25436, 22815,  1010,  1996,  2177,  2150,  2028,  1997,  1996,\n          2088,  1005,  1055,  2190,  1011,  4855,  2611,  2967,  1997,  2035,\n          2051,  1012,  2037, 14221,  2387,  1996,  2713,  1997, 20773,  1005,\n          1055,  2834,  2201,  1010, 20754,  1999,  2293,  1006,  2494,  1007,\n          1010,  2029,  2511,  2014,  2004,  1037,  3948,  3063,  4969,  1010,\n          3687,  2274,  8922,  2982,  1998,  2956,  1996,  4908,  2980,  2531,\n          2193,  1011,  2028,  3895,  1000,  4689,  1999,  2293,  1000,  1998,\n          1000,  3336,  2879,  1000,  1012,   102,  2043,  2106, 20773,  2707,\n          3352,  2759,  1029,   102,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0]),\n 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n         0, 0, 0, 0, 0, 0, 0, 0]),\n 'start_positions': tensor(67),\n 'end_positions': tensor(70)}"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset[0][\"start_positions\"]","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:54:24.554053Z","iopub.execute_input":"2024-06-18T04:54:24.555074Z","iopub.status.idle":"2024-06-18T04:54:24.562543Z","shell.execute_reply.started":"2024-06-18T04:54:24.555033Z","shell.execute_reply":"2024-06-18T04:54:24.561093Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"tensor(67)"},"metadata":{}}]},{"cell_type":"code","source":"print(train_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:03:35.647962Z","iopub.execute_input":"2024-06-17T10:03:35.648634Z","iopub.status.idle":"2024-06-17T10:03:35.653556Z","shell.execute_reply.started":"2024-06-17T10:03:35.648600Z","shell.execute_reply":"2024-06-17T10:03:35.652540Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"<__main__.SquadDataset object at 0x7aa140901f90>\n","output_type":"stream"}]},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:56:30.895465Z","iopub.execute_input":"2024-06-18T04:56:30.896250Z","iopub.status.idle":"2024-06-18T04:56:30.901501Z","shell.execute_reply.started":"2024-06-18T04:56:30.896197Z","shell.execute_reply":"2024-06-18T04:56:30.900430Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"for batch_idx, batch in enumerate(train_loader):\n    if batch_idx == 0:\n        print(batch)\n        break","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:56:36.318516Z","iopub.execute_input":"2024-06-18T04:56:36.318910Z","iopub.status.idle":"2024-06-18T04:56:36.364024Z","shell.execute_reply.started":"2024-06-18T04:56:36.318878Z","shell.execute_reply":"2024-06-18T04:56:36.362803Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"{'input_ids': tensor([[ 101, 1999, 2054,  ...,    0,    0,    0],\n        [ 101, 1996, 2682,  ...,    0,    0,    0],\n        [ 101, 9829, 3742,  ...,    0,    0,    0],\n        ...,\n        [ 101, 1996, 8720,  ...,    0,    0,    0],\n        [ 101, 2006, 2238,  ...,    0,    0,    0],\n        [ 101, 1037, 2969,  ...,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        ...,\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0],\n        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]]), 'start_positions': tensor([101,   7,  52, 159,  56,   7,  62, 114]), 'end_positions': tensor([105,   7,  69, 159,  60,   7,  68, 114])}\n","output_type":"stream"}]},{"cell_type":"code","source":"input_ids = torch.tensor(train_encodings['input_ids']).unsqueeze(0)\nsegment_ids = torch.tensor(train_encodings['token_type_ids']).unsqueeze(0)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:03:36.966816Z","iopub.execute_input":"2024-06-17T10:03:36.967416Z","iopub.status.idle":"2024-06-17T10:03:40.463394Z","shell.execute_reply.started":"2024-06-17T10:03:36.967388Z","shell.execute_reply":"2024-06-17T10:03:40.462395Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport time\nfrom transformers import BertForQuestionAnswering, BertTokenizer\n\n# Initialize BERT model and tokenizer\nmodel_name = 'bert-base-uncased'\nmodel = BertForQuestionAnswering.from_pretrained(model_name).to(device)\ntokenizer = BertTokenizer.from_pretrained(model_name)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:57:37.823740Z","iopub.execute_input":"2024-06-18T04:57:37.824119Z","iopub.status.idle":"2024-06-18T04:57:45.438463Z","shell.execute_reply.started":"2024-06-18T04:57:37.824086Z","shell.execute_reply":"2024-06-18T04:57:45.437423Z"},"trusted":true},"execution_count":34,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f4cc15e63a4476b990019c4fd42f254"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AdamW\noptim = AdamW(model.parameters(), lr=5e-5)\n\nepochs = 3\ndevice = torch.device('cuda:0' if torch.cuda.is_available()\n                      else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2024-06-18T04:57:49.053607Z","iopub.execute_input":"2024-06-18T04:57:49.053995Z","iopub.status.idle":"2024-06-18T04:57:49.076480Z","shell.execute_reply.started":"2024-06-18T04:57:49.053965Z","shell.execute_reply":"2024-06-18T04:57:49.075293Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"# whole_train_eval_time = time.time()\n\n# train_losses = []\n# val_losses = []\n\n# print_every = 1000\n\n# for epoch in range(epochs):\n#     epoch_time = time.time()\n    \n#     # Set model in train mode\n#     model.train()\n#     loss_of_epoch = 0\n    \n#     print(\"############Train############\")\n    \n#     for batch_idx,batch in enumerate(train_loader): \n#         optim.zero_grad()\n        \n#         input_ids = batch['input_ids'].to(device)\n#         attention_mask = batch['attention_mask'].to(device)\n#         start_positions = batch['start_positions'].to(device)\n#         end_positions = batch['end_positions'].to(device)\n        \n#         outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n#         loss = outputs[0]\n#         # do a backwards pass \n#         loss.backward()\n#         # update the weights\n#         optim.step()\n#         # Find the total loss\n#         loss_of_epoch += loss.item()\n        \n#         if (batch_idx+1) % print_every == 0:\n#             print(\"Batch {:} / {:}\".format(batch_idx+1,len(train_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n        \n#     loss_of_epoch /= len(train_loader)\n#     train_losses.append(loss_of_epoch)\n    \n#     ##########Evaluation##################\n    \n#     # Set model in evaluation mode\n#     model.eval()\n    \n#     print(\"############Evaluate############\")\n    \n#     loss_of_epoch = 0\n    \n#     for batch_idx,batch in enumerate(val_loader):\n        \n#         with torch.no_grad():\n            \n#             input_ids = batch['input_ids'].to(device)\n#             attention_mask = batch['attention_mask'].to(device)\n#             start_positions = batch['start_positions'].to(device)\n#             end_positions = batch['end_positions'].to(device)\n            \n#             outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n#             loss = outputs[0]\n#             # Find the total loss\n#             loss_of_epoch += loss.item()\n            \n#         if (batch_idx+1) % print_every == 0:\n#             print(\"Batch {:} / {:}\".format(batch_idx+1,len(val_loader)),\"\\nLoss:\", round(loss.item(),1),\"\\n\")\n    \n#     loss_of_epoch /= len(val_loader)\n#     val_losses.append(loss_of_epoch)\n    \n#     # Print each epoch's time and train/val loss \n    \n#     print(\"\\n-------Epoch \", epoch+1,\n#           \"-------\"\n#           \"\\nTraining Loss:\", train_losses[-1],\n#           \"\\nValidation Loss:\", val_losses[-1],\n#           \"\\nTime: \",(time.time() - epoch_time),\n#           \"\\n-----------------------\",\n#           \"\\n\\n\")\n\n# print(\"Total training and evaluation time: \", (time.time() - whole_train_eval_time))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:03:47.590258Z","iopub.execute_input":"2024-06-17T10:03:47.590984Z","iopub.status.idle":"2024-06-17T10:03:47.597484Z","shell.execute_reply.started":"2024-06-17T10:03:47.590949Z","shell.execute_reply":"2024-06-17T10:03:47.596381Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# Example Illustration\n# Let's illustrate this with a simplified example:\n# Start Logits: These are scores or logits corresponding to each token in the input sequence, indicating the likelihood that each token is the start of the answer span.\n# End Logits: Similarly, these logits indicate the likelihood that each token is the end of the answer span\n# Suppose the input sequence has 10 tokens.\n# BERT predicts start_logits and end_logits for each token.\n# The ground truth indicates that the answer span starts at index 3 and ends at index 5.\n# For simplicity, assume hypothetical logits:\n\n# start_logits = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n# end_logits = [0.5, 0.4, 0.3, 0.2, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6]\n# start_positions = 3 (index where answer span starts)\n# end_positions = 5 (index where answer span ends)\n# Calculation:\n\n# start_loss = CrossEntropyLoss([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], 3)\n# end_loss = CrossEntropyLoss([0.5, 0.4, 0.3, 0.2, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6], 5)\n# total_loss = start_loss + end_loss\n# Conclusion\n# In summary, the start and end logits are probabilities for each token indicating the likelihood that \n# it represents the start or end of the answer span. The loss function (CrossEntropyLoss in this case) quantifies the error between predicted logits and true positions. By minimizing this \n# loss during training, the model learns \n# to better predict accurate answer spans for given questions and contexts.","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install peft","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:03:48.489707Z","iopub.execute_input":"2024-06-17T10:03:48.490339Z","iopub.status.idle":"2024-06-17T10:04:00.844797Z","shell.execute_reply.started":"2024-06-17T10:03:48.490306Z","shell.execute_reply":"2024-06-17T10:04:00.843629Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.11.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader, random_split\nfrom transformers import BertTokenizerFast, BertForQuestionAnswering, BertConfig\nfrom tqdm import tqdm\nimport timeit\nimport os\n\nif __name__ == \"__main__\":\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n    LEARNING_RATE = 5e-5\n    BATCH_SIZE = 4\n    EPOCHS = 3  # Train for 3 epochs\n    MODEL_PATH = \"bert-base-uncased\"\n    MODEL_SAVE_PATH = \"/kaggle/working/models/\"\n\n    # Assuming you have defined train_dataset and val_dataset\n    train_loader = DataLoader(dataset=train_dataset,\n                              batch_size=BATCH_SIZE,\n                              shuffle=True)\n    \n    val_loader = DataLoader(dataset=val_dataset,\n                            batch_size=BATCH_SIZE,\n                            shuffle=False)  # No need to shuffle validation data\n    \n    # Load tokenizer and model configuration from the pretrained model\n    tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n    config = BertConfig.from_pretrained(MODEL_PATH)\n    \n    # Load model with the same configuration as the pretrained model\n    model = BertForQuestionAnswering(config).to(device)\n    \n    optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)\n\n    # Training loop\n    for epoch in range(EPOCHS):\n        print(f\"Epoch {epoch + 1}/{EPOCHS}\")\n        print('-' * 20)\n        \n        # Training phase\n        model.train()\n        train_running_loss = 0\n        for batch in tqdm(train_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            start_positions = batch['start_positions'].to(device)\n            end_positions = batch['end_positions'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n                            start_positions=start_positions, end_positions=end_positions)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n\n            train_running_loss += loss.item()\n\n        train_loss = train_running_loss / len(train_loader)\n        print(f\"Train Loss: {train_loss:.4f}\")\n\n        # Validation phase\n        model.eval()\n        val_running_loss = 0\n        with torch.no_grad():\n            for batch in tqdm(val_loader):\n                input_ids = batch['input_ids'].to(device)\n                attention_mask = batch['attention_mask'].to(device)\n                start_positions = batch['start_positions'].to(device)\n                end_positions = batch['end_positions'].to(device)\n\n                outputs = model(input_ids=input_ids, attention_mask=attention_mask,\n                                start_positions=start_positions, end_positions=end_positions)\n\n                val_running_loss += outputs.loss.item()\n\n        val_loss = val_running_loss / len(val_loader)\n        print(f\"Validation Loss: {val_loss:.4f}\")\n\n        print('-' * 20)\n\n    # Save the fine-tuned model and tokenizer\n    model.save_pretrained(MODEL_SAVE_PATH)\n    tokenizer.save_pretrained(MODEL_SAVE_PATH)\n\n    # Verify saved files\n    print(f\"Saved files: {os.listdir(MODEL_SAVE_PATH)}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:04:08.827357Z","iopub.execute_input":"2024-06-17T10:04:08.828180Z","iopub.status.idle":"2024-06-17T10:38:03.944638Z","shell.execute_reply.started":"2024-06-17T10:04:08.828139Z","shell.execute_reply":"2024-06-17T10:38:03.943597Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/3\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2713/2713 [10:42<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 4.3377\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 635/635 [00:34<00:00, 18.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 4.3072\n--------------------\nEpoch 2/3\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2713/2713 [10:42<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.8352\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 635/635 [00:34<00:00, 18.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 4.6373\n--------------------\nEpoch 3/3\n--------------------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2713/2713 [10:42<00:00,  4.22it/s]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 3.6193\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 635/635 [00:34<00:00, 18.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Validation Loss: 4.5364\n--------------------\nSaved files: ['vocab.txt', 'tokenizer_config.json', 'config.json', 'special_tokens_map.json', 'tokenizer.json', 'model.safetensors']\n","output_type":"stream"}]},{"cell_type":"code","source":"# Define the path to the SQuAD 2.0 training data\npath = \"/kaggle/input/squad-dataset-mee/train-v2.0.json\"\n\n# Load and preprocess the SQuAD 2.0 data\ndef load_squad_data(path):\n    with open(path, 'rb') as f:\n        squad_dict = json.load(f)\n\n    texts = []\n    questions = []\n    answers = []\n\n    for group in squad_dict['data']:\n        for passage in group['paragraphs']:\n            context = passage['context']\n            for qa in passage['qas']:\n                question = qa['question']\n                for answer in qa['answers']:\n                    texts.append(context)\n                    questions.append(question)\n                    answers.append(answer)\n\n    return texts[10850:20000], questions[10850:20000], answers[10850:20000]\n\n# Preprocess the data to find answer start and end positions\ntrain_texts, train_queries, train_answers = load_squad_data(path)\nfor answer, text in zip(train_answers, train_texts):\n    real_answer = answer['text']\n    start_idx = answer['answer_start']\n    # Get the real end index\n    end_idx = start_idx + len(real_answer)\n\n    # Deal with the problem of 1 or 2 more characters\n    if text[start_idx:end_idx] == real_answer:\n        answer['answer_end'] = end_idx\n    # When the real answer is more by one character\n    elif text[start_idx-1:end_idx-1] == real_answer:\n        answer['answer_start'] = start_idx - 1\n        answer['answer_end'] = end_idx - 1\n    # When the real answer is more by two characters\n    elif text[start_idx-2:end_idx-2] == real_answer:\n        answer['answer_start'] = start_idx - 2\n        answer['answer_end'] = end_idx - 2\n\ntrain_encodings = tokenizer(train_texts, train_queries, truncation=True, padding = True)\ndef add_token_positions(encodings, answers):\n    start_positions = []\n    end_positions = []\n    \n    count = 0\n    \n    for i in range(len(answers)):\n        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end']))\n        \n        # if start position is None, the answer passage has been truncated\n        if start_positions[-1] is None:\n            start_positions[-1] = tokenizer.model_max_length\n            \n        # if end position is None, the 'char_to_token' function points to the space after the correct token, so add - 1\n        if end_positions[-1] is None:\n            end_positions[-1] = encodings.char_to_token(i, answers[i]['answer_end'] - 1)\n            \n            # if end position is still None the answer passage has been truncated\n            if end_positions[-1] is None:\n                count += 1\n                end_positions[-1] = tokenizer.model_max_length\n    print(count)\n    \n    # Update the data in dictionary\n    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n\nadd_token_positions(train_encodings, train_answers)\nadd_token_positions(val_encodings, val_answers)\nclass SquadDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings):\n        self.encodings = encodings\n\n    def __getitem__(self, idx):\n        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n\n    def __len__(self):\n        return len(self.encodings.input_ids)\ntes_dataset = SquadDataset(train_encodings)\ntest_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:34:13.793599Z","iopub.execute_input":"2024-06-17T11:34:13.794040Z","iopub.status.idle":"2024-06-17T11:34:17.680883Z","shell.execute_reply.started":"2024-06-17T11:34:13.794010Z","shell.execute_reply":"2024-06-17T11:34:17.679973Z"},"trusted":true},"execution_count":77,"outputs":[{"name":"stdout","text":"0\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"# Testing the model\nfrom sklearn.metrics import f1_score\npreds = []\ntrue = []\nwith torch.no_grad():\n    for idx, sample in enumerate(tqdm(test_loader)):\n        input_ids = sample['input_ids'].to(device)\n        attention_mask = sample['attention_mask'].to(device)\n        start_positions = sample['start_positions']\n        end_positions = sample['end_positions']\n\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n\n        start_pred = torch.argmax(outputs['start_logits'], dim=1).cpu().detach()\n        end_pred = torch.argmax(outputs['end_logits'], dim=1).cpu().detach()\n\n        preds.extend([[int(i), int(j)] for i, j in zip(start_pred, end_pred)])\n        true.extend([[int(i), int(j)] for i, j in zip(start_positions, end_positions)])\n\npreds = [item for sublist in preds for item in sublist]\ntrue = [item for sublist in true for item in sublist]\nf1_value = f1_score(true, preds, average=\"macro\")\nprint(f\"F1 Score: {f1_value}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:38:15.362892Z","iopub.execute_input":"2024-06-17T11:38:15.363721Z","iopub.status.idle":"2024-06-17T11:40:43.367140Z","shell.execute_reply.started":"2024-06-17T11:38:15.363690Z","shell.execute_reply":"2024-06-17T11:40:43.366067Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"100%|██████████| 1144/1144 [02:27<00:00,  7.76it/s]","output_type":"stream"},{"name":"stdout","text":"F1 Score: 0.016194817412470978\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"from torch.utils.data import DataLoader, Dataset","metadata":{"execution":{"iopub.status.busy":"2024-06-17T10:53:48.101862Z","iopub.execute_input":"2024-06-17T10:53:48.102697Z","iopub.status.idle":"2024-06-17T10:53:48.107328Z","shell.execute_reply.started":"2024-06-17T10:53:48.102665Z","shell.execute_reply":"2024-06-17T10:53:48.106236Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"\nMODEL_SAVE_PATH = \"/kaggle/working/models/\"\n\n# Load tokenizer\ntokenizer = BertTokenizerFast.from_pretrained(MODEL_SAVE_PATH)\nmodel = BertForQuestionAnswering.from_pretrained(MODEL_SAVE_PATH).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:06:40.761416Z","iopub.execute_input":"2024-06-17T11:06:40.761791Z","iopub.status.idle":"2024-06-17T11:06:41.131241Z","shell.execute_reply.started":"2024-06-17T11:06:40.761763Z","shell.execute_reply":"2024-06-17T11:06:41.130463Z"},"trusted":true},"execution_count":54,"outputs":[]},{"cell_type":"code","source":"model = BertForQuestionAnswering.from_pretrained(MODEL_SAVE_PATH)\nmodel.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nmodel.eval()","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:06:34.031673Z","iopub.execute_input":"2024-06-17T11:06:34.032314Z","iopub.status.idle":"2024-06-17T11:06:34.636706Z","shell.execute_reply.started":"2024-06-17T11:06:34.032284Z","shell.execute_reply":"2024-06-17T11:06:34.635806Z"},"trusted":true},"execution_count":53,"outputs":[{"execution_count":53,"output_type":"execute_result","data":{"text/plain":"BertForQuestionAnswering(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def predict(question, context):\n    # Tokenize inputs\n    inputs = tokenizer.encode_plus(question, context, return_tensors='pt')\n    \n    # Extract input IDs and attention mask\n    input_ids = inputs[\"input_ids\"].to(device)\n    attention_mask = inputs[\"attention_mask\"].to(device)\n    \n    # Perform the model forward pass\n    with torch.no_grad():\n        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n        start_logits = outputs.start_logits\n        end_logits = outputs.end_logits\n    \n    # Get the most likely start and end positions\n    start_index = torch.argmax(start_logits)\n    end_index = torch.argmax(end_logits) + 1  # Add 1 to include the end token\n    \n    # Retrieve the answer span and convert tokens back to string\n    answer = tokenizer.decode(input_ids[0][start_index:end_index])\n    return answer\n","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:06:47.639373Z","iopub.execute_input":"2024-06-17T11:06:47.639999Z","iopub.status.idle":"2024-06-17T11:06:47.646751Z","shell.execute_reply.started":"2024-06-17T11:06:47.639968Z","shell.execute_reply":"2024-06-17T11:06:47.645852Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"Context = \"William Shakespeare was an English playwright, poet, and actor, widely regarded as the greatest writer in the English language.\"\nQuestion = \"Who is considered the greatest writer in the English language?\"\nprint(predict(Question,Context))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:08:01.033261Z","iopub.execute_input":"2024-06-17T11:08:01.034125Z","iopub.status.idle":"2024-06-17T11:08:01.051047Z","shell.execute_reply.started":"2024-06-17T11:08:01.034081Z","shell.execute_reply":"2024-06-17T11:08:01.050242Z"},"trusted":true},"execution_count":57,"outputs":[{"name":"stdout","text":"william shakespeare was an english playwright\n","output_type":"stream"}]},{"cell_type":"code","source":"Context = \"The moon orbits the Earth approximately once every 27.3 days.\"\nQuestion = \"How long does it take for the moon to orbit the Earth?\"\nprint(predict(Question,Context))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:09:40.847895Z","iopub.execute_input":"2024-06-17T11:09:40.848388Z","iopub.status.idle":"2024-06-17T11:09:40.864089Z","shell.execute_reply.started":"2024-06-17T11:09:40.848358Z","shell.execute_reply":"2024-06-17T11:09:40.863194Z"},"trusted":true},"execution_count":60,"outputs":[{"name":"stdout","text":"orbits the earth approximately once every 27. 3 days.\n","output_type":"stream"}]},{"cell_type":"code","source":"Context = \"The Great Barrier Reef is the world's largest coral reef system composed of over 2,900 individual reefs.\"\nQuestion = \"What is the Great Barrier Reef?\"\nprint(predict(Question,Context))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:10:33.094138Z","iopub.execute_input":"2024-06-17T11:10:33.095010Z","iopub.status.idle":"2024-06-17T11:10:33.110531Z","shell.execute_reply.started":"2024-06-17T11:10:33.094976Z","shell.execute_reply":"2024-06-17T11:10:33.109566Z"},"trusted":true},"execution_count":61,"outputs":[{"name":"stdout","text":"barrier reef is the world's largest coral reef system composed of over 2, 900 individual reefs\n","output_type":"stream"}]},{"cell_type":"code","source":"Context = \"The Declaration of Independence was adopted on July 4, 1776, declaring the independence of the United States from Great Britain.\"\nQuestion = \"On what date was the Declaration of Independence adopted?\"\nprint(predict(Question,Context))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:14:01.420523Z","iopub.execute_input":"2024-06-17T11:14:01.421214Z","iopub.status.idle":"2024-06-17T11:14:01.436577Z","shell.execute_reply.started":"2024-06-17T11:14:01.421182Z","shell.execute_reply":"2024-06-17T11:14:01.435710Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"july 4,\n","output_type":"stream"}]},{"cell_type":"code","source":"Context = \"The theory of general relativity, formulated by Albert Einstein, revolutionized our understanding of gravity.\"\nQuestion = \"What scientific theory did Albert Einstein formulate?\"\nprint(predict(Question,Context))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:15:16.826025Z","iopub.execute_input":"2024-06-17T11:15:16.826447Z","iopub.status.idle":"2024-06-17T11:15:16.843831Z","shell.execute_reply.started":"2024-06-17T11:15:16.826415Z","shell.execute_reply":"2024-06-17T11:15:16.842840Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"gravity.\n","output_type":"stream"}]},{"cell_type":"code","source":"a = \"The Hubble Space Telescope, launched into orbit in 1990, has provided unprecedented views of distant galaxies.\"\nb = \"When was the Hubble Space Telescope launched?\"\nprint(predict(a,b))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:16:32.000437Z","iopub.execute_input":"2024-06-17T11:16:32.001104Z","iopub.status.idle":"2024-06-17T11:16:32.019290Z","shell.execute_reply.started":"2024-06-17T11:16:32.001071Z","shell.execute_reply":"2024-06-17T11:16:32.018395Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"1990, has provided unprecedented views of distant galaxies\n","output_type":"stream"}]},{"cell_type":"code","source":"a = \"The Renaissance, a period of cultural and intellectual flourishing in Europe, spanned from the 14th to the 17th century.\"\nb = \"What historical period is known for its cultural and intellectual flourishing in Europe?\"\nprint(predict(a,b))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:18:00.908460Z","iopub.execute_input":"2024-06-17T11:18:00.909122Z","iopub.status.idle":"2024-06-17T11:18:00.925265Z","shell.execute_reply.started":"2024-06-17T11:18:00.909078Z","shell.execute_reply":"2024-06-17T11:18:00.924264Z"},"trusted":true},"execution_count":69,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"a = \"The Industrial Revolution, beginning in the late 18th century, brought about significant social and economic changes in Europe.\"\nb = \"What historical event led to major social and economic transformations in Europe during the late 18th century?\"\nprint(predict(a,b))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:19:17.356534Z","iopub.execute_input":"2024-06-17T11:19:17.357414Z","iopub.status.idle":"2024-06-17T11:19:17.374853Z","shell.execute_reply.started":"2024-06-17T11:19:17.357371Z","shell.execute_reply":"2024-06-17T11:19:17.374068Z"},"trusted":true},"execution_count":70,"outputs":[{"name":"stdout","text":"18th century, brought about significant social and economic changes in europe. [SEP] what historical event led to major social and economic transformations in europe during the late 18th\n","output_type":"stream"}]},{"cell_type":"code","source":"a = \" Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"\nb = \"When did Beyonce start becoming popular?\"\nprint(predict(a,b))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:21:45.611911Z","iopub.execute_input":"2024-06-17T11:21:45.612612Z","iopub.status.idle":"2024-06-17T11:21:45.631137Z","shell.execute_reply.started":"2024-06-17T11:21:45.612580Z","shell.execute_reply":"2024-06-17T11:21:45.630260Z"},"trusted":true},"execution_count":72,"outputs":[{"name":"stdout","text":"dangerously in love ( 2003 ), which established her as a solo artist worldwide, earned five\n","output_type":"stream"}]},{"cell_type":"code","source":"a = \"The Normans (Norman: Nourmands; French: Normands; Latin: Normanni) were the people who in the 10th and 11th centuries gave their name to Normandy, a region in France. They were descended from Norse ('Norman' comes from 'Norseman') raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-Gaulish populations, their descendants would gradually merge with the Carolingian-based cultures of West Francia. The distinct cultural and ethnic identity of the Normans emerged initially in the first half of the 10th century, and it continued to evolve over the succeeding centuries.\"\nb =  \"In what country is Normandy located?\"\nprint(predict(a,b))","metadata":{"execution":{"iopub.status.busy":"2024-06-17T11:22:58.962198Z","iopub.execute_input":"2024-06-17T11:22:58.962814Z","iopub.status.idle":"2024-06-17T11:22:58.981275Z","shell.execute_reply.started":"2024-06-17T11:22:58.962786Z","shell.execute_reply":"2024-06-17T11:22:58.980500Z"},"trusted":true},"execution_count":74,"outputs":[{"name":"stdout","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}